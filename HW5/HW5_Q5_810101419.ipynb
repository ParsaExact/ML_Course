{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 5\n",
        "### Parsa Daghigh\n",
        "### Std num: 810101419\n"
      ],
      "metadata": {
        "id": "44oaLovtLN_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "DKTeKuHSLaRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyJzo27qFlPv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load train and test"
      ],
      "metadata": {
        "id": "Yz8bkIXaLl8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "oeB605QtLgIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part a\n",
        "### How to handle missing data?\n",
        "\n",
        "1. **Deleting Rows or Columns Containing Missing Data:**\n",
        "   - Simple to implement and avoids introducing bias in the data.\n",
        "   - Useful information may be lost.\n",
        "\n",
        "2. **Replacing with Mean or Median Value:**\n",
        "   -  Easy to implement and retains all data.\n",
        "   -  May reduce model accuracy as it decreases data variability.\n",
        "\n",
        "3. **Replacing with Predictions from Machine Learning Models:**\n",
        "   -  Preserves data variability and increases model accuracy.\n",
        "   -  More complex to implement and requires more computations.\n",
        "\n",
        "4. **Using Methods Based on most frequency:**\n",
        "   -  Can reduce data variability and decrease model accuracy."
      ],
      "metadata": {
        "id": "WBWvm4ioLym3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B , C , D"
      ],
      "metadata": {
        "id": "uPd9R7y5SKdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percent = train_data.isnull().mean() * 100\n",
        "\n",
        "missing_percent_non_zero = missing_percent[missing_percent > 0]\n",
        "\n",
        "print(\"Features with missing values and their percentages:\")\n",
        "print(missing_percent_non_zero)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQMWyF5bLqTQ",
        "outputId": "e8df7f5e-e647-4fc5-eed9-0fd5a74e7d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features with missing values and their percentages:\n",
            "LotFrontage     18.590998\n",
            "Alley           93.542074\n",
            "MasVnrType      57.729941\n",
            "MasVnrArea       0.293542\n",
            "BsmtQual         2.544031\n",
            "BsmtCond         2.544031\n",
            "BsmtExposure     2.544031\n",
            "BsmtFinType1     2.544031\n",
            "BsmtFinType2     2.544031\n",
            "Electrical       0.097847\n",
            "FireplaceQu     47.651663\n",
            "GarageType       5.283757\n",
            "GarageYrBlt      5.283757\n",
            "GarageFinish     5.283757\n",
            "GarageQual       5.283757\n",
            "GarageCond       5.283757\n",
            "PoolQC          99.510763\n",
            "Fence           80.234834\n",
            "MiscFeature     96.086106\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 50\n",
        "\n",
        "# Filter out features with missing values above the threshold\n",
        "features_to_drop = missing_percent[missing_percent > threshold].index\n",
        "train_data = train_data.drop(columns=features_to_drop)\n",
        "test_data = test_data.drop(columns=features_to_drop)\n",
        "\n",
        "print(\"Features dropped due to high percentage of missing values:\")\n",
        "print(features_to_drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAiXHqH6QiRn",
        "outputId": "d59d559a-c22f-4c4d-8ed2-7def601f4894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features dropped due to high percentage of missing values:\n",
            "Index(['Alley', 'MasVnrType', 'PoolQC', 'Fence', 'MiscFeature'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data_train = train_data.select_dtypes(include=['number'])\n",
        "categorical_data_train = train_data.select_dtypes(include=['object'])\n",
        "\n",
        "numeric_data_test = test_data.select_dtypes(include=['number'])\n",
        "categorical_data_test = test_data.select_dtypes(include=['object'])\n",
        "\n",
        "# Impute numeric data with mean\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "numeric_data_train_imputed = pd.DataFrame(imputer_mean.fit_transform(numeric_data_train), columns=numeric_data_train.columns)\n",
        "numeric_data_test_imputed = pd.DataFrame(imputer_mean.transform(numeric_data_test), columns=numeric_data_test.columns)\n",
        "\n",
        "# Impute categorical data with mode\n",
        "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "categorical_data_train_imputed = pd.DataFrame(imputer_mode.fit_transform(categorical_data_train), columns=categorical_data_train.columns)\n",
        "categorical_data_test_imputed = pd.DataFrame(imputer_mode.transform(categorical_data_test), columns=categorical_data_test.columns)\n",
        "\n",
        "# One-hot encode categorical data\n",
        "encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "combined_categorical_data = pd.concat([categorical_data_train_imputed, categorical_data_test_imputed], axis=0)\n",
        "encoder.fit(combined_categorical_data)\n",
        "\n",
        "categorical_encoded_train = encoder.transform(categorical_data_train_imputed)\n",
        "categorical_encoded_test = encoder.transform(categorical_data_test_imputed)\n",
        "\n",
        "categorical_encoded_train_df = pd.DataFrame(categorical_encoded_train, columns=encoder.get_feature_names_out(categorical_data_train_imputed.columns))\n",
        "categorical_encoded_test_df = pd.DataFrame(categorical_encoded_test, columns=encoder.get_feature_names_out(categorical_data_test_imputed.columns))\n",
        "\n",
        "train_data_imputed = pd.concat([numeric_data_train_imputed, categorical_encoded_train_df], axis=1)\n",
        "test_data_imputed = pd.concat([numeric_data_test_imputed, categorical_encoded_test_df], axis=1)\n",
        "\n",
        "print(\"Imputation and encoding completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPTtL-vsRVji",
        "outputId": "c78b3ea3-4dc6-4c51-cba4-638592bc4a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputation and encoding completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize numeric features\n",
        "scaler = StandardScaler()\n",
        "scaled_features_train = scaler.fit_transform(numeric_data_train_imputed)\n",
        "scaled_features_test = scaler.transform(numeric_data_test_imputed)\n",
        "\n",
        "processed_data_train = pd.concat([pd.DataFrame(scaled_features_train, columns=numeric_data_train_imputed.columns), categorical_encoded_train_df.reset_index(drop=True)], axis=1)\n",
        "processed_data_test = pd.concat([pd.DataFrame(scaled_features_test, columns=numeric_data_test_imputed.columns), categorical_encoded_test_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(\"Normalization and encoding completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zcEZVHSSeLG",
        "outputId": "e9ed7b32-f359-4ccb-b05e-4cc64e77515d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization and encoding completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part E\n",
        "### Create MLP model"
      ],
      "metadata": {
        "id": "bhDYXPJAWoZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.drop1 = nn.Dropout(p = 0.1)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.drop2 = nn.Dropout(p = 0.1)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "X_train_tensor = torch.tensor(processed_data_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(train_data['SalePrice'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "X_test_tensor = torch.tensor(processed_data_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(test_data['SalePrice'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "input_dim = processed_data_train.shape[1]\n",
        "model = MLP(input_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93wTB72hTlBK",
        "outputId": "3ab5c466-4973-4731-a1ad-0760b00e9758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/500], Loss: 33210914816.0000\n",
            "Epoch [20/500], Loss: 1793515136.0000\n",
            "Epoch [30/500], Loss: 542255104.0000\n",
            "Epoch [40/500], Loss: 689383680.0000\n",
            "Epoch [50/500], Loss: 469448992.0000\n",
            "Epoch [60/500], Loss: 609746304.0000\n",
            "Epoch [70/500], Loss: 317065216.0000\n",
            "Epoch [80/500], Loss: 1000863808.0000\n",
            "Epoch [90/500], Loss: 667606656.0000\n",
            "Epoch [100/500], Loss: 294871776.0000\n",
            "Epoch [110/500], Loss: 387219904.0000\n",
            "Epoch [120/500], Loss: 425847264.0000\n",
            "Epoch [130/500], Loss: 466012800.0000\n",
            "Epoch [140/500], Loss: 212836384.0000\n",
            "Epoch [150/500], Loss: 1128283776.0000\n",
            "Epoch [160/500], Loss: 1190421888.0000\n",
            "Epoch [170/500], Loss: 811154176.0000\n",
            "Epoch [180/500], Loss: 560931136.0000\n",
            "Epoch [190/500], Loss: 317669952.0000\n",
            "Epoch [200/500], Loss: 350498112.0000\n",
            "Epoch [210/500], Loss: 224013920.0000\n",
            "Epoch [220/500], Loss: 414451712.0000\n",
            "Epoch [230/500], Loss: 302182496.0000\n",
            "Epoch [240/500], Loss: 1056753408.0000\n",
            "Epoch [250/500], Loss: 230686928.0000\n",
            "Epoch [260/500], Loss: 384697952.0000\n",
            "Epoch [270/500], Loss: 274046496.0000\n",
            "Epoch [280/500], Loss: 358485696.0000\n",
            "Epoch [290/500], Loss: 341829312.0000\n",
            "Epoch [300/500], Loss: 142869952.0000\n",
            "Epoch [310/500], Loss: 171910720.0000\n",
            "Epoch [320/500], Loss: 353120064.0000\n",
            "Epoch [330/500], Loss: 189249008.0000\n",
            "Epoch [340/500], Loss: 204423536.0000\n",
            "Epoch [350/500], Loss: 348545984.0000\n",
            "Epoch [360/500], Loss: 281036384.0000\n",
            "Epoch [370/500], Loss: 154848704.0000\n",
            "Epoch [380/500], Loss: 298046784.0000\n",
            "Epoch [390/500], Loss: 295451680.0000\n",
            "Epoch [400/500], Loss: 143888560.0000\n",
            "Epoch [410/500], Loss: 199671872.0000\n",
            "Epoch [420/500], Loss: 132454840.0000\n",
            "Epoch [430/500], Loss: 111383264.0000\n",
            "Epoch [440/500], Loss: 128412088.0000\n",
            "Epoch [450/500], Loss: 110946328.0000\n",
            "Epoch [460/500], Loss: 183662640.0000\n",
            "Epoch [470/500], Loss: 143500144.0000\n",
            "Epoch [480/500], Loss: 156267280.0000\n",
            "Epoch [490/500], Loss: 257896864.0000\n",
            "Epoch [500/500], Loss: 174135824.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part E\n",
        "#### Evaluate the model"
      ],
      "metadata": {
        "id": "KZTnZFp0WNzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsle(y_true, y_pred):\n",
        "    log_true = np.log1p(y_true)\n",
        "    log_pred = np.log1p(y_pred)\n",
        "    return np.sqrt(mean_squared_error(log_true, log_pred))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions_train = model(X_train_tensor).numpy()\n",
        "    predictions_test = model(X_test_tensor).numpy()\n",
        "\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_tensor.numpy(), predictions_train))\n",
        "mae_train = mean_absolute_error(y_train_tensor.numpy(), predictions_train)\n",
        "mape_train = np.mean(np.abs((y_train_tensor.numpy() - predictions_train) / y_train_tensor.numpy())) * 100\n",
        "rmsle_train = rmsle(y_train_tensor.numpy(), predictions_train)\n",
        "\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_tensor.numpy(), predictions_test))\n",
        "mae_test = mean_absolute_error(y_test_tensor.numpy(), predictions_test)\n",
        "mape_test = np.mean(np.abs((y_test_tensor.numpy() - predictions_test) / y_test_tensor.numpy())) * 100\n",
        "rmsle_test = rmsle(y_test_tensor.numpy(), predictions_test)\n",
        "\n",
        "print(f'Training RMSE: {rmse_train:.3f}')\n",
        "print(f'Training MAE: {mae_train:.3f}')\n",
        "print(f'Training MAPE: {mape_train:.3f}%')\n",
        "print(f'Training RMSLE: {rmsle_train:.3f}')\n",
        "\n",
        "print(f'Test RMSE: {rmse_test:.3f}')\n",
        "print(f'Test MAE: {mae_test:.3f}')\n",
        "print(f'Test MAPE: {mape_test:.3f}%')\n",
        "print(f'Test RMSLE: {rmsle_test:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuwGldy8VfuY",
        "outputId": "42d0964c-93fe-41ab-c1c4-fe64da0e2c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 4891.429\n",
            "Training MAE: 3762.096\n",
            "Training MAPE: 2.427%\n",
            "Training RMSLE: 0.037\n",
            "Test RMSE: 6138.407\n",
            "Test MAE: 4664.661\n",
            "Test MAPE: 3.168%\n",
            "Test RMSLE: 0.057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part F\n",
        "#### Analysis of Results:\n",
        "\n",
        "- **Training Set:**\n",
        "  - **RMSE**: 4715.463\n",
        "  - **MAE**: 3529.637\n",
        "  - **MAPE**: 2.269%\n",
        "  - **RMSLE**: 0.034\n",
        "  \n",
        "- **Test Set:**\n",
        "  - **RMSE**: 6165.644\n",
        "  - **MAE**: 4615.916\n",
        "  - **MAPE**: 3.092%\n",
        "  - **RMSLE**: 0.049\n",
        "\n",
        "These values indicate that my model has effectively learned from the training data and performs well on the test data. The relatively low differences between the training and test error metrics suggest that your model is generalizing well to unseen data.\n",
        "\n",
        "#### Overfitting and Underfitting Analysis:\n",
        "Based on the results, my model shows a good balance between fitting the training data and generalizing to the test data. There are no significant signs of overfitting or underfitting. However, the following suggestions can help to further improve your model's performance:\n",
        "\n",
        "### Recommendations to Reduce Overfitting or Underfitting:\n",
        "1. **Cross-Validation**: Use cross-validation to evaluate the model's performance and prevent overfitting.\n",
        "2. **Gathering More Data**: Collect more data if possible to enhance model training.\n",
        "3. **Regularization**: Implement regularization techniques such as L1 and L2 regularization.\n",
        "4. **Model Development**: Experiment with adding or removing layers and neurons to optimize the model architecture.\n",
        "5. **Ensembling**: Combine multiple models to improve accuracy and stability of predictions."
      ],
      "metadata": {
        "id": "g3MT5hYHaFAx"
      }
    }
  ]
}