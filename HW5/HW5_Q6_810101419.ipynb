{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 6\n",
        "## Parsa Daghigh\n",
        "## Std. Num : 810101419\n",
        "\n",
        "### Part A: CNNs and Translation Invariance\n",
        "Convolutional Neural Networks (CNNs) are a class of deep learning models particularly well-suited for image processing tasks.\n",
        "\n",
        "#### **Convolutional Neural Networks (CNNs)**\n",
        "- Convolutional Layers: These layers apply convolution operations to the input, extracting local patterns such as edges, textures, and shapes.\n",
        "\n",
        "- Pooling Layers: These layers perform down-sampling operations, reducing the spatial dimensions of the data and helping to achieve translation invariance.\n",
        "\n",
        "- Fully Connected Layers: These layers connect every neuron in one layer to every neuron in the next layer, similar to traditional neural networks.\n",
        "\n",
        "#### **Translation Invariance**\n",
        "Translation invariance means that the model's ability to recognize objects or features is robust to shifts in the input image. For example, a CNN should be able to recognize a cat in an image regardless of where it appears. <br>\n",
        "\n",
        "This property allows CNNs to be more robust and effective in tasks where the position of objects can vary, such as image classification and object detection."
      ],
      "metadata": {
        "id": "aq4zKXj6c39A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B: Components Contributing to Translation Invariance\n",
        "\n",
        "- Convolutional Layers: By applying filters to local regions, convolutional layers capture spatial hierarchies of features that are invariant to position.\n",
        "\n",
        "- Pooling Layers: Max pooling or average pooling layers reduce the spatial dimensions of the data, making the model less sensitive to the exact position of features.\n",
        "\n",
        "- ReLU Activation: The use of ReLU activation functions introduces non-linearity, which helps the network learn more complex patterns.\n",
        "\n",
        "- Strided Convolutions: Convolution operations with a stride greater than one help in achieving translational invariance by skipping over some positions in the input.\n",
        "\n",
        "- Data Augmentation: Techniques like random cropping, rotation, and translation of images during training can further enhance translation invariance."
      ],
      "metadata": {
        "id": "fkWWb-H-eJWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part C : MLP for MNIST dataset"
      ],
      "metadata": {
        "id": "TI-leOV6edfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "FEDWu4Bxfn8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0LhcTJrccENF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "mlp_model = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    mlp_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = mlp_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCi20jtQgE5G",
        "outputId": "b08ec7bd-161e-4c48-9e43-037c2e71f6c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:11<00:00, 899kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 63.7kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 241kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.14MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/10], Accuracy: 95.12%\n",
            "Epoch [2/10], Accuracy: 96.64%\n",
            "Epoch [3/10], Accuracy: 96.49%\n",
            "Epoch [4/10], Accuracy: 97.56%\n",
            "Epoch [5/10], Accuracy: 97.31%\n",
            "Epoch [6/10], Accuracy: 96.53%\n",
            "Epoch [7/10], Accuracy: 97.09%\n",
            "Epoch [8/10], Accuracy: 97.57%\n",
            "Epoch [9/10], Accuracy: 97.38%\n",
            "Epoch [10/10], Accuracy: 97.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "lenet_model = LeNet5()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lenet_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    lenet_model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = lenet_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "lenet_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        outputs = lenet_model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg6tgllq1WyS",
        "outputId": "2f078e27-072e-4c9d-d135-33c50437f7e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4986\n",
            "Epoch [2/20], Loss: 0.2091\n",
            "Epoch [3/20], Loss: 0.0469\n",
            "Epoch [4/20], Loss: 0.1186\n",
            "Epoch [5/20], Loss: 0.1849\n",
            "Epoch [6/20], Loss: 0.0101\n",
            "Epoch [7/20], Loss: 0.0068\n",
            "Epoch [8/20], Loss: 0.0203\n",
            "Epoch [9/20], Loss: 0.0669\n",
            "Epoch [10/20], Loss: 0.2684\n",
            "Epoch [11/20], Loss: 0.0571\n",
            "Epoch [12/20], Loss: 0.0137\n",
            "Epoch [13/20], Loss: 0.0179\n",
            "Epoch [14/20], Loss: 0.0556\n",
            "Epoch [15/20], Loss: 0.0048\n",
            "Epoch [16/20], Loss: 0.0060\n",
            "Epoch [17/20], Loss: 0.1018\n",
            "Epoch [18/20], Loss: 0.0076\n",
            "Epoch [19/20], Loss: 0.1067\n",
            "Epoch [20/20], Loss: 0.0169\n",
            "Test Accuracy: 98.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part E and F: Residual Neural Networks (ResNet)\n",
        "Advantages:\n",
        "- Addressing Vanishing/Exploding Gradient Problem: Traditional deep networks suffer from vanishing or exploding gradients as they become deeper. ResNets address this issue by using residual connections, allowing gradients to flow directly through the network.\n",
        "\n",
        "- Ease of Training: Residual connections make it easier to train very deep networks (e.g., hundreds of layers) by providing shortcut paths for gradients during backpropagation.\n",
        "\n",
        "- Improved Accuracy: ResNets have achieved state-of-the-art performance on various benchmarks, including ImageNet, by allowing the construction of much deeper networks without degradation in accuracy.\n",
        "\n",
        "- Effective Feature Learning: The network can learn more abstract and high-level features as depth increases, leading to improved performance on complex tasks.\n",
        "\n",
        "#### **Unique Components of ResNet Compared to Traditional CNNs:**<br>\n",
        "Residual Blocks: The key component of ResNet is the residual block, which includes identity connections (shortcut connections) that bypass one or more layers. This allows the network to learn residual functions rather than directly learning the underlying mapping.<br>\n",
        "\n",
        "Identity Connection: The identity connection (shortcut) allows the input to be directly added to the output of the convolutional layers, ensuring that gradients can flow smoothly through the network.<br>\n"
      ],
      "metadata": {
        "id": "2R5oQROccCIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Modify ResNet-18 for MNIST\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(weights=None)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "resnet_model = ModifiedResNet18()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    resnet_model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = resnet_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "resnet_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        outputs = resnet_model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPSZhvIZdIsN",
        "outputId": "ee45acbc-44de-4500-a853-dc4399d455c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2905\n",
            "Epoch [2/10], Loss: 0.1376\n",
            "Epoch [3/10], Loss: 0.0133\n",
            "Epoch [4/10], Loss: 0.0002\n",
            "Epoch [5/10], Loss: 0.0213\n",
            "Epoch [6/10], Loss: 0.0059\n",
            "Epoch [7/10], Loss: 0.0631\n",
            "Epoch [8/10], Loss: 0.1254\n",
            "Epoch [9/10], Loss: 0.0005\n",
            "Epoch [10/10], Loss: 0.0021\n",
            "Test Accuracy: 99.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part G"
      ],
      "metadata": {
        "id": "umSzm-GCfa1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Select an image\n",
        "sample_idx = 0\n",
        "sample_image, sample_label = test_dataset[sample_idx]\n",
        "\n",
        "# original image\n",
        "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
        "plt.title(f'Original Image - Label: {sample_label}')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "3U7t1GvAenim",
        "outputId": "bf106b9e-9b4b-4977-a5a8-5a6fc348508b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cab9e1360dfb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1307\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.3081\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Select an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shift_pixels = 5\n",
        "shifted_image = TF.affine(sample_image, angle=0, translate=(shift_pixels, 0), scale=1, shear=0)\n",
        "\n",
        "# shifted image\n",
        "plt.imshow(shifted_image.squeeze(), cmap='gray')\n",
        "plt.title(f'Shifted Image - {shift_pixels} pixels to the right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NBenJ4F1e0bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model.eval()\n",
        "lenet_model.eval()\n",
        "resnet_model.eval()\n",
        "\n",
        "shifted_image = shifted_image.unsqueeze(0)\n",
        "\n",
        "# Predict using MLP\n",
        "with torch.no_grad():\n",
        "    mlp_prediction = mlp_model(shifted_image)\n",
        "    mlp_predicted_digit = torch.argmax(mlp_prediction, dim=1).item()\n",
        "\n",
        "# Predict using LeNet\n",
        "with torch.no_grad():\n",
        "    lenet_prediction = lenet_model(shifted_image)\n",
        "    lenet_predicted_digit = torch.argmax(lenet_prediction, dim=1).item()\n",
        "\n",
        "# Predict using ResNet\n",
        "with torch.no_grad():\n",
        "    resnet_prediction = resnet_model(shifted_image)\n",
        "    resnet_predicted_digit = torch.argmax(resnet_prediction, dim=1).item()\n",
        "\n",
        "print(f'MLP Predicted Digit: {mlp_predicted_digit}')\n",
        "print(f'LeNet Predicted Digit: {lenet_predicted_digit}')\n",
        "print(f'ResNet Predicted Digit: {resnet_predicted_digit}')\n"
      ],
      "metadata": {
        "id": "x_o5XvMvgiK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}